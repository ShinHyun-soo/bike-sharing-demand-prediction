라이브러리 로딩
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
train = pd.read_csv('data/train.csv') 
test = pd.read_csv('data/test.csv')
결측치 확인 및 처리
train.isnull().sum()
id                          0
hour                        0
hour_bef_temperature        2
hour_bef_precipitation      2
hour_bef_windspeed          9
hour_bef_humidity           2
hour_bef_visibility         2
hour_bef_ozone             76
hour_bef_pm10              90
hour_bef_pm2.5            117
count                       0
dtype: int64
test.isnull().sum()
id                         0
hour                       0
hour_bef_temperature       1
hour_bef_precipitation     1
hour_bef_windspeed         1
hour_bef_humidity          1
hour_bef_visibility        1
hour_bef_ozone            35
hour_bef_pm10             37
hour_bef_pm2.5            36
dtype: int64
train.fillna(0,inplace = True)
test.fillna(0,inplace = True)
model=RandomForestRegressor(n_estimators=42)
from pprint import pprint
pprint(model.get_params())
{'bootstrap': True,
 'ccp_alpha': 0.0,
 'criterion': 'squared_error',
 'max_depth': None,
 'max_features': 'auto',
 'max_leaf_nodes': None,
 'max_samples': None,
 'min_impurity_decrease': 0.0,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'n_estimators': 42,
 'n_jobs': None,
 'oob_score': False,
 'random_state': None,
 'verbose': 0,
 'warm_start': False}
파라미터 튜닝
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)
{'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
{'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
{'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 4320, cv = 9, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(train_x, train_y)
Fitting 9 folds for each of 4320 candidates, totalling 38880 fits
RandomizedSearchCV(cv=9, estimator=RandomForestRegressor(), n_iter=4320,
                   n_jobs=-1,
                   param_distributions={'bootstrap': [True, False],
                                        'max_depth': [10, 20, 30, 40, 50, 60,
                                                      70, 80, 90, 100, 110,
                                                      None],
                                        'max_features': ['auto', 'sqrt'],
                                        'min_samples_leaf': [1, 2, 4],
                                        'min_samples_split': [2, 5, 10],
                                        'n_estimators': [200, 400, 600, 800,
                                                         1000, 1200, 1400, 1600,
                                                         1800, 2000]},
                   random_state=42, verbose=2)
rf_random.best_params_
{'n_estimators': 200,
 'min_samples_split': 2,
 'min_samples_leaf': 1,
 'max_features': 'auto',
 'max_depth': 110,
 'bootstrap': True}
rf = RandomForestRegressor(n_estimators = 200,
                           min_samples_split = 2, 
                           min_samples_leaf = 1,
                           max_depth = 110, 
                           bootstrap = True)
모델 정의 및 학습
train_x = train.drop(['count'],axis = 1)
train_y = train['count']
rf.fit(train_x,train_y)
RandomForestRegressor(max_depth=110, n_estimators=200)
from xgboost import XGBRegressor
reg = XGBRegressor(n_estimators = 200,
                    max_depth = 110)
reg.fit(train_x, train_y)
pred = reg.predict(test)
학습 된 모델로 예측 데이터 생성
pred = rf.predict(test)
제출파일 생성
submission = pd.read_csv('C:/Users/User/ringringe/content/data/submission.csv')
submission
id	count
0	0	NaN
1	1	NaN
2	2	NaN
3	4	NaN
4	5	NaN
...	...	...
710	2148	NaN
711	2149	NaN
712	2165	NaN
713	2166	NaN
714	2177	NaN
715 rows × 2 columns

submission['count'] = pred
submission
id	count
0	0	110.437843
1	1	187.968155
2	2	56.289230
3	4	23.666124
4	5	85.705978
...	...	...
710	2148	41.078442
711	2149	67.621460
712	2165	90.824585
713	2166	160.606400
714	2177	128.306046
715 rows × 2 columns

submission.to_csv('베이스라인.csv',index = False)
